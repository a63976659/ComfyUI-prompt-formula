import torch
import numpy as np
from PIL import Image, ImageOps
import folder_paths
import os

class å›¾åƒè£å‰ªèŠ‚ç‚¹:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        input_dir = folder_paths.get_input_directory()
        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]
        return {
            "required": {
                "å›¾åƒæ–‡ä»¶": (sorted(files), {"image_upload": True}),
                "é¢„è®¾": (["è‡ªå®šä¹‰", "1:1", "2:3", "3:2", "3:4", "4:3", "9:16", "16:9"], {"default": "è‡ªå®šä¹‰"}),
                "è£å‰ªå®½åº¦": ("FLOAT", {"default": 512, "min": 16, "max": 16384, "step": 16, "display": "slider"}),
                "è£å‰ªé«˜åº¦": ("FLOAT", {"default": 512, "min": 16, "max": 16384, "step": 16, "display": "slider"}),
                "è£å‰ªX": ("INT", {"default": 0, "min": 0, "max": 16384, "step": 1, "display": "number"}),
                "è£å‰ªY": ("INT", {"default": 0, "min": 0, "max": 16384, "step": 1, "display": "number"}),
                # å°†ç¼©æ”¾æ¯”ä¾‹ç§»åŠ¨åˆ°æœ€åï¼Œä»¥ä¾¿åœ¨ UI ä¸Šæ˜¾ç¤ºåœ¨åº•éƒ¨
                "ç¼©æ”¾æ¯”ä¾‹": ("FLOAT", {"default": 1.0, "min": 0.1, "max": 1.0, "step": 0.01, "display": "slider"}),
            }
        }

    RETURN_TYPES = ("IMAGE", "INT", "INT")
    RETURN_NAMES = ("å›¾åƒ", "å®½åº¦", "é«˜åº¦")
    FUNCTION = "do_crop"
    CATEGORY = "ğŸ“•æç¤ºè¯å…¬å¼/å·¥å…·èŠ‚ç‚¹"
    DESCRIPTION = "â„¹ï¸å…¶å®ƒé¢„è®¾ç¼©æ”¾ä¼šæ­£å¸¸ç”Ÿæ•ˆ \n è‡ªå®šä¹‰æ¨¡å¼ä¸‹ï¼Œæé™å®½é«˜åº¦ï¼ŒæŒ‰æ¯”ä¾‹ç¼©æ”¾ä¼šæœ‰å¼‚å¸¸ï¼Œè¯·è°¨æ…ä½¿ç”¨ï¼"

    def do_crop(self, å›¾åƒæ–‡ä»¶, é¢„è®¾, ç¼©æ”¾æ¯”ä¾‹, è£å‰ªå®½åº¦, è£å‰ªé«˜åº¦, è£å‰ªX, è£å‰ªY):
        img_path = folder_paths.get_annotated_filepath(å›¾åƒæ–‡ä»¶)
        img = Image.open(img_path)
        img = ImageOps.exif_transpose(img)
        img = img.convert("RGB")
        
        orig_w, orig_h = img.size
        
        # 1. å°ºå¯¸å¤„ç† (ä½¿ç”¨å‰ç«¯ä¼ æ¥çš„å€¼)
        target_w = int(è£å‰ªå®½åº¦)
        target_h = int(è£å‰ªé«˜åº¦)

        # å¼ºåˆ¶ 16 å€æ•°å¯¹é½
        target_w = max(16, (target_w // 16) * 16)
        target_h = max(16, (target_h // 16) * 16)

        # å°ºå¯¸è¾¹ç•Œé™åˆ¶
        target_w = min(target_w, (orig_w // 16) * 16)
        target_h = min(target_h, (orig_h // 16) * 16)
        
        # 2. åæ ‡å¤„ç†
        x = int(è£å‰ªX)
        y = int(è£å‰ªY)

        # åæ ‡è¾¹ç•Œé™åˆ¶ (é˜²æ­¢çº¢æ¡†è·‘å‡ºå›¾åƒå¤–)
        x = max(0, min(x, orig_w - target_w))
        y = max(0, min(y, orig_h - target_h))
        
        # 3. æ‰§è¡Œè£å‰ª
        cropped_img = img.crop((x, y, x + target_w, y + target_h))
        
        # 4. è½¬ Tensor
        img_np = np.array(cropped_img).astype(np.float32) / 255.0
        img_tensor = torch.from_numpy(img_np)[None,]
        
        return {
            "ui": {"images": []},
            "result": (img_tensor, target_w, target_h)
        }