# -*- coding: utf-8 -*-
import torch
import comfy.utils
import math

MAX_RESOLUTION = 16384

# ================= æ ¸å¿ƒç®—æ³•å‡½æ•° =================
def get_size(target):
    if target is None: return 0, 0
    if len(target.shape) == 4: return target.shape[2], target.shape[1]
    return target.shape[2], target.shape[1]

def common_upscale(samples, width, height, method, crop):
    if width == 0 and height == 0: return samples
    is_image = len(samples.shape) == 4
    if is_image: samples = samples.movedim(-1, 1)
    else: samples = samples.unsqueeze(1)
    s = comfy.utils.common_upscale(samples, width, height, method, crop)
    if is_image: return s.movedim(1, -1)
    return s.squeeze(1)

def scale_by(samples, multiplier, method):
    w, h = get_size(samples)
    return common_upscale(samples, round(w * multiplier), round(h * multiplier), method, "disabled")

def scale_dimensions(samples, width, height, method, crop):
    w, h = get_size(samples)
    if width == 0: width = max(1, round(w * height / h))
    elif height == 0: height = max(1, round(h * width / w))
    return common_upscale(samples, width, height, method, crop)

def scale_longer(samples, size, method):
    w, h = get_size(samples)
    if w >= h: t_w, t_h = size, round(h * size / w)
    else: t_h, t_w = size, round(w * size / h)
    return common_upscale(samples, t_w, t_h, method, "disabled")

def scale_shorter(samples, size, method):
    w, h = get_size(samples)
    if w <= h: t_w, t_h = size, round(h * size / w)
    else: t_h, t_w = size, round(w * size / h)
    return common_upscale(samples, t_w, t_h, method, "disabled")

def scale_total_pixels(samples, megapixels, method):
    w, h = get_size(samples)
    total = int(megapixels * 1024 * 1024)
    scale = math.sqrt(total / (w * h))
    return common_upscale(samples, round(w * scale), round(h * scale), method, "disabled")

def scale_match(samples, match, method, crop):
    if match is None: return samples
    t_w, t_h = get_size(match)
    return common_upscale(samples, t_w, t_h, method, crop)

def scale_to_multiple(samples, multiple, method):
    if multiple <= 1: return samples
    w, h = get_size(samples)
    t_w = (w // multiple) * multiple
    t_h = (h // multiple) * multiple
    if t_w == 0 or t_h == 0: return samples
    
    s_w = t_w / w
    s_h = t_h / h
    if s_w >= s_h:
        sc_w = t_w
        sc_h = int(math.ceil(h * s_w))
        if sc_h < t_h: sc_h = t_h
    else:
        sc_h = t_h
        sc_w = int(math.ceil(w * s_h))
        if sc_w < t_w: sc_w = t_w
        
    scaled = common_upscale(samples, sc_w, sc_h, method, "disabled")
    x0 = (sc_w - t_w) // 2
    y0 = (sc_h - t_h) // 2
    
    if len(scaled.shape) == 4: return scaled[:, y0:y0+t_h, x0:x0+t_w, :]
    else: return scaled[:, y0:y0+t_h, x0:x0+t_w]

# ================= èŠ‚ç‚¹å®šä¹‰ =================
class æ™ºèƒ½ç¼©æ”¾å›¾åƒåŠé®ç½©:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "è°ƒæ•´ç±»åž‹": ([
                    "æŒ‡å®šå°ºå¯¸ (Dimensions)", 
                    "æŒ‰ç³»æ•° (Scale By)", 
                    "æŒ‡å®šé•¿è¾¹ (Longer Side)", 
                    "æŒ‡å®šçŸ­è¾¹ (Shorter Side)", 
                    "æŒ‡å®šå®½åº¦ (Width)", 
                    "æŒ‡å®šé«˜åº¦ (Height)", 
                    "æŒ‡å®šåƒç´  (Megapixels)", 
                    "åŒ¹é…å›¾åƒ (Match Size)", 
                    "ä¹˜æ•°è°ƒæ•´ (To Multiple)"
                ], {"default": "æŒ‡å®šå°ºå¯¸ (Dimensions)"}),
                "æ’å€¼æ–¹æ³•": (["nearest-exact", "bilinear", "area", "bicubic", "lanczos"], {"default": "area"}),
            },
            "optional": {
                "å›¾åƒæˆ–é®ç½©": ("*",), 
                "å‚è€ƒå›¾åƒ": ("*",), 
                "è£å‰ªæ–¹å¼": (["disabled", "center"], {"default": "center"}),
                "å®½åº¦": ("INT", {"default": 512, "min": 0, "max": MAX_RESOLUTION, "step": 1}),
                "é«˜åº¦": ("INT", {"default": 512, "min": 0, "max": MAX_RESOLUTION, "step": 1}),
                "ç¼©æ”¾ç³»æ•°": ("FLOAT", {"default": 1.0, "min": 0.01, "max": 100.0, "step": 0.01}),
                "æŒ‡å®šé•¿åº¦": ("INT", {"default": 1024, "min": 1, "max": MAX_RESOLUTION, "step": 1}), 
                "ç™¾ä¸‡åƒç´ ": ("FLOAT", {"default": 1.0, "min": 0.01, "max": 100.0, "step": 0.01}),
                "æ”¾å¤§": ("INT", {"default": 8, "min": 1, "max": 512, "step": 1}), 
            }
        }

    # ã€ä¿®æ”¹ã€‘è¾“å‡ºç±»åž‹æ”¹ä¸ºé€šç”¨çš„ "*"
    RETURN_TYPES = ("*",)
    # ã€ä¿®æ”¹ã€‘è¾“å‡ºåç§°åˆå¹¶
    RETURN_NAMES = ("å›¾åƒ/é®ç½©",)
    FUNCTION = "execute"
    CATEGORY = "ðŸ“•æç¤ºè¯å…¬å¼/å·¥å…·èŠ‚ç‚¹"
    
    DESCRIPTION = """
    è°ƒæ•´ç±»åž‹åŠŸèƒ½è¯´æ˜Žï¼š
    1. æŒ‡å®šå°ºå¯¸/å®½åº¦/é«˜åº¦ï¼šå¼ºåˆ¶ç¼©æ”¾åˆ°ç‰¹å®šåƒç´ ã€‚
    2. æŒ‰ç³»æ•°ï¼šæŒ‰å€çŽ‡ç¼©æ”¾ï¼ˆå¦‚ 0.5 æˆ– 2.0ï¼‰ã€‚
    3. æŒ‡å®šé•¿/çŸ­è¾¹ï¼šé”å®šä¸€è¾¹é•¿åº¦ï¼Œå¦ä¸€è¾¹æŒ‰æ¯”ä¾‹è‡ªé€‚åº”ã€‚
    4. æŒ‡å®šåƒç´ ï¼šä¿æŒæ¯”ä¾‹ï¼Œå°†æ€»åƒç´ æ•°è°ƒæ•´åˆ°è¿‘ä¼¼å€¼ï¼ˆå¦‚ 1MPï¼‰ã€‚
    5. åŒ¹é…å›¾åƒï¼šå°†è¾“å…¥å›¾åƒè°ƒæ•´ä¸ºä¸Žå‚è€ƒå›¾åƒä¸€è‡´çš„å¤§å°ã€‚
    6. ä¹˜æ•°è°ƒæ•´ (To Multiple)ï¼šã€é‡è¦ã€‘å°†å›¾åƒé•¿å®½è°ƒæ•´ä¸ºæŒ‡å®šæ•°å€¼ï¼ˆå¦‚8ï¼‰çš„æ•´æ•°å€ã€‚
       - ä½œç”¨ï¼šä¿®å¤éžæ ‡å‡†å°ºå¯¸å›¾åƒï¼Œé˜²æ­¢ SD VAE æˆ–è§†é¢‘æ¨¡åž‹å› å°ºå¯¸æ— æ³•æ•´é™¤è€ŒæŠ¥é”™ã€‚
       - ç®—æ³•ï¼šä¿æŒæ¯”ä¾‹ç¼©æ”¾è¦†ç›–ç›®æ ‡å°ºå¯¸ï¼Œå¹¶è¿›è¡Œå±…ä¸­è£å‰ªã€‚
    """

    def execute(self, è°ƒæ•´ç±»åž‹, æ’å€¼æ–¹æ³•, å›¾åƒæˆ–é®ç½©=None, å‚è€ƒå›¾åƒ=None, è£å‰ªæ–¹å¼="center", å®½åº¦=512, é«˜åº¦=512, ç¼©æ”¾ç³»æ•°=1.0, æŒ‡å®šé•¿åº¦=1024, ç™¾ä¸‡åƒç´ =1.0, æ”¾å¤§=8):
        target = å›¾åƒæˆ–é®ç½©
        # å¦‚æžœæ²¡æœ‰è¾“å…¥ï¼Œè¿”å›žç©ºçš„å…ƒç»„
        if target is None: return (None,)

        def process_one(sample):
            if sample is None: return None
            # ä¸¥æ ¼å¯¹åº”å‰ç«¯é€‰é¡¹å­—ç¬¦ä¸²
            if "æŒ‡å®šå°ºå¯¸" in è°ƒæ•´ç±»åž‹: return scale_dimensions(sample, å®½åº¦, é«˜åº¦, æ’å€¼æ–¹æ³•, è£å‰ªæ–¹å¼)
            elif "æŒ‰ç³»æ•°" in è°ƒæ•´ç±»åž‹: return scale_by(sample, ç¼©æ”¾ç³»æ•°, æ’å€¼æ–¹æ³•)
            elif "æŒ‡å®šé•¿è¾¹" in è°ƒæ•´ç±»åž‹: return scale_longer(sample, æŒ‡å®šé•¿åº¦, æ’å€¼æ–¹æ³•)
            elif "æŒ‡å®šçŸ­è¾¹" in è°ƒæ•´ç±»åž‹: return scale_shorter(sample, æŒ‡å®šé•¿åº¦, æ’å€¼æ–¹æ³•)
            elif "æŒ‡å®šå®½åº¦" in è°ƒæ•´ç±»åž‹: return scale_dimensions(sample, å®½åº¦, 0, æ’å€¼æ–¹æ³•, "disabled")
            elif "æŒ‡å®šé«˜åº¦" in è°ƒæ•´ç±»åž‹: return scale_dimensions(sample, 0, é«˜åº¦, æ’å€¼æ–¹æ³•, "disabled")
            elif "æŒ‡å®šåƒç´ " in è°ƒæ•´ç±»åž‹: return scale_total_pixels(sample, ç™¾ä¸‡åƒç´ , æ’å€¼æ–¹æ³•)
            elif "åŒ¹é…å›¾åƒ" in è°ƒæ•´ç±»åž‹: return scale_match(sample, å‚è€ƒå›¾åƒ, æ’å€¼æ–¹æ³•, è£å‰ªæ–¹å¼)
            elif "ä¹˜æ•°è°ƒæ•´" in è°ƒæ•´ç±»åž‹: return scale_to_multiple(sample, æ”¾å¤§, æ’å€¼æ–¹æ³•)
            return sample

        # ã€ä¿®æ”¹ã€‘ä¸å†åŒºåˆ† Image/Mask è¿”å›žï¼Œç»Ÿä¸€å¤„ç†å¹¶è¿”å›ž
        result = process_one(target)
        return (result,)